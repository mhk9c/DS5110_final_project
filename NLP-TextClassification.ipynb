{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "installing package 1\n",
      "installing package 2\n",
      "installing package 3\n",
      "installing package 4\n",
      "installing package 5\n",
      "installing package 6\n",
      "Done Installing packages\n"
     ]
    }
   ],
   "source": [
    "from utils_nlp import Tools\n",
    "tools = Tools('mhk9c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "spark = sparknlp.start(gpu=True, memory=\"256G\") \n",
    "# spark = sparknlp.start() \n",
    "# sparknlp.start(gpu=True) >> for training on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark NLP version 3.4.2\n",
      "Apache Spark version: 3.1.2\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from pyspark.ml import Pipeline\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "print(\"Spark NLP version\", sparknlp.version())\n",
    "print(\"Apache Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading from /project/ds5559/team1_sp22/data//russian-troll-tweets-enriched.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1467572"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_data(_df, seed = 314, train_test = [0.7, 0.3]):\n",
    "    # Doesn't this create info leakage? The test set has knowledge of train's mean and standard deviation from the above scaling?    \n",
    "    train_data, test_data = _df.randomSplit(train_test, seed=seed)\n",
    "    return train_data, test_data\n",
    "\n",
    "_df = tools.load_data(spark, \"russian-troll-tweets-enriched\")\n",
    "trainDataset, testDataset = split_data(_df)\n",
    "trainDataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|label| count|\n",
      "+-----+------+\n",
      "|    1|784380|\n",
      "|    0|683192|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "trainDataset.groupBy(\"label\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfhub_use download started this may take some time.\n",
      "Approximate size to download 923.7 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# actual content is inside description column\n",
    "document = DocumentAssembler()\\\n",
    "    .setInputCol(\"curated_content\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "    \n",
    "# we can also use sentence detector here \n",
    "# if we want to train on and get predictions for each sentence\n",
    "# downloading pretrained embeddings\n",
    "use = UniversalSentenceEncoder.pretrained()\\\n",
    " .setInputCols([\"document\"])\\\n",
    " .setOutputCol(\"sentence_embeddings\")\n",
    "# the classes/labels/categories are in category column\n",
    "classsifierdl = ClassifierDLApproach()\\\n",
    "  .setInputCols([\"sentence_embeddings\"])\\\n",
    "  .setOutputCol(\"class\")\\\n",
    "  .setLabelColumn(\"label\")\\\n",
    "  .setMaxEpochs(5)\\\n",
    "  .setEnableOutputLogs(True)\n",
    "use_clf_pipeline = Pipeline(\n",
    "    stages = [\n",
    "        document,\n",
    "        use,\n",
    "        classsifierdl\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_pipelineModel = use_clf_pipeline.fit(trainDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = use_pipelineModel.transform(testDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------------------------------------------------------------------+------+\n",
      "|label|                                                                 curated_content|result|\n",
      "+-----+--------------------------------------------------------------------------------+------+\n",
      "|    0|                                                     #np GHADIMI – Absent Lights|   [1]|\n",
      "|    0|                                                   '@zeleness Downtime – misery'|   [0]|\n",
      "|    0|10 weekly followers. 47 unfollowers. Crowdfire doesn't miss a trick - via htt...|   [1]|\n",
      "|    0|4 weekly followers. 7 unfollowers. Crowdfire doesn't miss a trick - via http:...|   [1]|\n",
      "|    1|\"\"\"Don't just let your business or your job make something for you; let it ma...|   [1]|\n",
      "|    1|\"\"\"I have been very consistent..\"\" https://t.co/Qqifn2Fe9F @TheView @megynkel...|   [1]|\n",
      "|    1|\"\"\"My record proves my deep and long-lasting support for Israel.\"\" So does th...|   [1]|\n",
      "|    1|\"\"\"The presidency is not a TV show\"\" https://t.co/jKEcJwZgcb https://t.co/9s9...|   [1]|\n",
      "|    1|                           \"\"\"You got the juice\"\"   Me: https://t.co/TBj8UVOWWU\"|   [0]|\n",
      "|    1|\"#BeingBlackIs being told you \"\"talk like a white person\"\" when you're articu...|   [1]|\n",
      "+-----+--------------------------------------------------------------------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds.select('label', 'curated_content', 'class.result').show(10, truncate=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = use_pipelineModel.transform(testDataset).select('label', 'curated_content', 'class.result').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['result'] = df['result'].apply(lambda x :x[0])\n",
    "df['label'] = df['label'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label              object\n",
       "curated_content    object\n",
       "result             object\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.84      0.85    292627\n",
      "           1       0.86      0.88      0.87    335849\n",
      "\n",
      "    accuracy                           0.86    628476\n",
      "   macro avg       0.86      0.86      0.86    628476\n",
      "weighted avg       0.86      0.86      0.86    628476\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df.label, df.result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8621124752576073\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(df.label, df.result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
