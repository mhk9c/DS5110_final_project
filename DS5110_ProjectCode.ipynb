{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DS 5110 Final Project\n",
    "#### Michael Kolonay (mhk9c)\n",
    "#### Tyler Entner (tje6gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Dataset Variable Descriptions:\n",
    "\n",
    "| Variable Name      | Description |\n",
    "| ----------- | ----------- |\n",
    "|**external_author_id**|\tAn author account ID from Twitter|\n",
    "|**author**\t|The handle sending the tweet\n",
    "|**content**\t|The text of the tweet\n",
    "|**region**|\tA region classification, as determined by Social Studio\n",
    "|**language**|\tThe language of the tweet\n",
    "|**publish_date**|\tThe date and time the tweet was sent\n",
    "|**harvested_date**|\tThe date and time the tweet was collected by Social Studio\n",
    "|**following**|\tThe number of accounts the handle was following at the time of the tweet\n",
    "|**followers**|\tThe number of followers the handle had at the time of the tweet\n",
    "|**updates**|\tThe number of ‚Äúupdate actions‚Äù on the account that authored the tweet, including tweets, retweets and likes\n",
    "|**post_type**|\tIndicates if the tweet was a retweet or a quote-tweet\n",
    "|**account_type**|\tSpecific account theme, as coded by Linvill and Warren\n",
    "|**retweet**|\tA binary indicator of whether or not the tweet is a retweet\n",
    "|**account_category**|\tGeneral account theme, as coded by Linvill and Warren\n",
    "|**new_june_2018**|\tA binary indicator of whether the handle was newly listed in June 2018\n",
    "|**alt_external_id**|\tReconstruction of author account ID from Twitter, derived from article_url variable and the first list provided to Congress\n",
    "|**tweet_id**|\tUnique id assigned by twitter to each status update, derived from article_url\n",
    "|**article_url**|\tLink to original tweet. Now redirects to \"Account Suspended\" page\n",
    "|**tco1_step1**|\tFirst redirect for the first http(s)://t.co/ link in a tweet, if it exists\n",
    "|**tco2_step1**|\tFirst redirect for the second http(s)://t.co/ link in a tweet, if it exists\n",
    "|**tco3_step1**|\tFirst redirect for the third http(s)://t.co/ link in a tweet, if it exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 1: Data Import and Preprocessing\n",
    "#### Data Ingestion\n",
    "\n",
    "First, we'll load all of the datasets into dataframes. Each dataset will have the same schema, so we can load multiple datasets with one call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import context manager: SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# import data types\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "#Create session with custom app name, grab context\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"team1_sp22_final_project\") \\\n",
    "        .config(\"spark.executor.memory\", '100g') \\\n",
    "        .config('spark.executor.cores', '20') \\\n",
    "        .config('spark.cores.max', '20') \\\n",
    "        .config(\"spark.driver.memory\",'100g') \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data directory\n",
    "data_directory = \"data/\"\n",
    "\n",
    "# Define custom schema of csv files\n",
    "schema = StructType([StructField('external_author_id', StringType(), True), \n",
    "                     StructField('author', StringType(), True),\n",
    "                     StructField('content', StringType(), True),\n",
    "                     StructField('region', StringType(), True),\n",
    "                     StructField('language', StringType(), True),\n",
    "                     StructField('publish_date', StringType(), True),\n",
    "                     StructField('harvested_date', StringType(), True),\n",
    "                     StructField('following', IntegerType(), True),\n",
    "                     StructField('followers', IntegerType(), True),\n",
    "                     StructField('updates', IntegerType(), True),\n",
    "                     StructField('post_type', StringType(), True),\n",
    "                     StructField('account_type', StringType(), True),\n",
    "                     StructField('retweet', IntegerType(), True),\n",
    "                     StructField('account_category', StringType(), True),\n",
    "                     StructField('new_june_2018', IntegerType(), True),\n",
    "                     StructField('alt_external_id', StringType(), True),\n",
    "                     StructField('tweet_id', StringType(), True),\n",
    "                     StructField('article_url', StringType(), True),\n",
    "                     StructField('tco1_step1', StringType(), True),\n",
    "                     StructField('tco2_step1', StringType(), True),\n",
    "                     StructField('tco3_step1', StringType(), True)                    \n",
    "                    ])\n",
    "\n",
    "# Create df by loading in all csv files in data_directory with schema\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\",\",\") \\\n",
    "    .schema(schema) \\\n",
    "    .load(data_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(external_author_id='1647045721', author='CARRIETHORNTHON', content='New Study Reveals Liberals Have A Lower Average IQ Than Conservatives http://t.co/B82NFpFSf6 WE ON TWITTER KNEW THIS ALREADY.', region='United States', language='English', publish_date='6/1/2015 22:04', harvested_date='6/1/2015 22:04', following=80, followers=207, updates=1193, post_type='RETWEET', account_type='Right', retweet=1, account_category='RightTroll', new_june_2018=0, alt_external_id='1647045721', tweet_id='605495107186364419', article_url='http://twitter.com/CarrieThornthon/statuses/605495107186364419', tco1_step1='http://gopthedailydose.com/2015/06/01/new-study-reveals-liberals-have-a-lower-average-iq-than-conservatives/', tco2_step1=None, tco3_step1=None),\n",
       " Row(external_author_id='1647045721', author='CARRIETHORNTHON', content='Lindsey Graham has an entirely reasonable position on climate change, sometimes http://t.co/kZMZka7Ja7 http://t.co/55juNRjwAT', region='United States', language='English', publish_date='6/1/2015 22:04', harvested_date='6/1/2015 22:04', following=80, followers=207, updates=1195, post_type='RETWEET', account_type='Right', retweet=1, account_category='RightTroll', new_june_2018=0, alt_external_id='1647045721', tweet_id='605495132540911616', article_url='http://twitter.com/CarrieThornthon/statuses/605495132540911616', tco1_step1='https://twitter.com/HuffPostPol/status/605465259441311745/photo/1', tco2_step1='http://huff.to/1KIrtyZ', tco3_step1=None)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter for english content only\n",
    "df_english = df.filter(df['language']=='English')\n",
    "df_english.take(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2946207 21\n",
      "2096049 21\n"
     ]
    }
   ],
   "source": [
    "# Some information on the dataset:\n",
    "print(df.count(), len(df.columns))\n",
    "print(df_english.count(), len(df_english.columns))\n",
    "df = df_english #replace df with english df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning\n",
    "Now that the data has been ingested into a single dataframe, we can begin to examine a couple fields and determine if any cleaning needs to be done. \n",
    "\n",
    "Ideas:\n",
    "- Extract URL to seperate column, replace with <url>\n",
    "- Replace emojis with <emoji>, count number into seperate column\n",
    "- Take publish date and extract hour, day, month, year\n",
    "- Columns for length of content in words, characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features\n",
    "import emoji\n",
    "import re\n",
    "import datetime\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.types import StringType, ArrayType, IntegerType\n",
    "import re\n",
    "rx_b = re.compile(r\"@[a-zA-Z0-9]+\")\n",
    "rx_url = re.compile(r\"(?:http|ftp|https):\\/\\/(?:[\\w_-]+(?:(?:\\.[\\w_-]+)+))(?:[\\w.,@?^=%&:\\/~+#-]*[\\w@?^=%&\\/~+#-])\")\n",
    "\n",
    "\n",
    "# *************************************************************\n",
    "def convert_emojii(string): \n",
    "    '''\n",
    "    convert emoji to string representation with demoji\n",
    "    '''\n",
    "    try:\n",
    "        return demoji.replace_with_desc(string, \":\")\n",
    "    except:\n",
    "        return \"COULD NOT CONVERT EMOJII\"\n",
    "convert_emojii_UDF = func.udf(lambda z:convert_emojii(z),StringType())   \n",
    "# test = convert_emojii(\"üêùüêùüêù\")   \n",
    "# print(test)\n",
    "\n",
    "# *************************************************************\n",
    "def extract_domain_information(url):  \n",
    "    '''\n",
    "    Extract domain information with tldextract\n",
    "        Attempts to get registered domain if not parses out domain information from url\n",
    "    '''\n",
    "    try:        \n",
    "        if(url):            \n",
    "            ext = tldextract.extract(url)\n",
    "            if(ext.registered_domain):                \n",
    "                return ext.registered_domain\n",
    "            else :                \n",
    "                return f'{ext.subdomain}.{ext.domain}.{ext.suffix}'                \n",
    "        else:            \n",
    "            return \"NA\"        \n",
    "    except Exception as e:        \n",
    "        return \"NA\"    \n",
    "extract_domain_information_UDF = func.udf(lambda z:extract_domain_information(z),StringType())   \n",
    "\n",
    "# *************************************************************\n",
    "def extract_handles(content): \n",
    "    '''\n",
    "        gets all the handles in the tweet of the form @[a-zA-Z0-9]+ and returns an array\n",
    "    '''\n",
    "    try:\n",
    "        if(content is not None):        \n",
    "            result = re.findall(rx_b, content) \n",
    "            return result\n",
    "        else:\n",
    "            return []\n",
    "    except:\n",
    "        return []    \n",
    "extract_handles_UDF = func.udf(lambda z:extract_handles(z),ArrayType(StringType(), True))   \n",
    "# test = extract_handles(\"Hi @MichelleObama , remember when you praised Harvey Weinstein as 'a wonderful human being, a good friend and a powerhouse.\")\n",
    "# print(test)\n",
    "\n",
    "# *************************************************************\n",
    "def count_emoji(string):\n",
    "    '''\n",
    "    Count number of emojis within a string\n",
    "    '''\n",
    "    if string:\n",
    "        return emoji.emoji_count(string)\n",
    "    else:\n",
    "        return 0\n",
    "count_emoji_udf = func.udf(lambda x: count_emoji(x), IntegerType())\n",
    "\n",
    "# *************************************************************\n",
    "def extract_emoji(string):\n",
    "    '''\n",
    "    Extract emojis by converting them to text\n",
    "    '''\n",
    "    if string:\n",
    "        return emoji.demojize(emoji.distinct_emoji_lis(string))\n",
    "    else:\n",
    "        return 'None'\n",
    "extract_emoji_udf = func.udf(lambda x: extract_emoji(x), StringType())\n",
    "\n",
    "# *************************************************************\n",
    "def extract_urls(string):\n",
    "    '''\n",
    "    Extract all urls in string\n",
    "    '''\n",
    "    if string:\n",
    "#         urls = re.findall('(?:(?:https?|ftp):\\\\/\\\\/)?[\\\\w/\\\\-?=%.]+\\\\.[\\\\w/\\\\-&?=%.]+', string)\n",
    "        urls = re.findall(rx_url, string)\n",
    "\n",
    "        return urls\n",
    "    else:\n",
    "        return 'None'\n",
    "extract_urls_udf = func.udf(lambda x: extract_urls(x), StringType())\n",
    "\n",
    "# *************************************************************\n",
    "def url_count(string):\n",
    "    '''\n",
    "    Count all urls in string\n",
    "    '''\n",
    "    if string:\n",
    "        return(len(extract_urls(string)))\n",
    "    else:\n",
    "        return 0\n",
    "url_count_udf = func.udf(lambda x: url_count(x), IntegerType())\n",
    "\n",
    "# *************************************************************\n",
    "def extract_url_parts(string):\n",
    "    '''\n",
    "    Return url in parts (https://stackoverflow.com/questions/27745/getting-parts-of-a-url-regex)\n",
    "    '''\n",
    "    if string:\n",
    "        return re.findall('^((http[s]?|ftp):\\/)?\\/?([^:\\/\\s]+)((\\/\\w+)*\\/)([\\w\\-\\.]+[^#?\\s]+)(.*)?(#[\\w\\-]+)?$', string)\n",
    "    else:\n",
    "        return 'None'\n",
    "\n",
    "def extract_urls_redirect_base(string_1, string_2, string_3):\n",
    "    '''\n",
    "    Call extract_url_parts and create a list of hosts from twitters redirect columns\n",
    "    '''\n",
    "    try:\n",
    "        host_list = ['', '', '']\n",
    "        if string_3:\n",
    "            url_parts = extract_url_parts(string_3)\n",
    "            host_list[2] = url_parts[0][2]\n",
    "        if string_2:\n",
    "            url_parts = extract_url_parts(string_2)\n",
    "            host_list[1] = url_parts[0][2]\n",
    "        if string_1:\n",
    "            url_parts = extract_url_parts(string_1)\n",
    "            host_list[0] = url_parts[0][2]\n",
    "        else:\n",
    "            return 'None'\n",
    "    except:\n",
    "        return 'None'\n",
    "    return host_list\n",
    "extract_urls_redirect_base_udf = func.udf(lambda x,y,z: extract_urls_redirect_base(x,y,z), StringType())\n",
    "\n",
    "# *************************************************************\n",
    "def word_count(string):\n",
    "    '''\n",
    "    Count number of words in string (slightly error prone b/c split on spaces)\n",
    "    '''\n",
    "    if string:\n",
    "        return len(string.split(' '))\n",
    "    else:\n",
    "        return 0\n",
    "word_count_udf = func.udf(lambda x: word_count(x), IntegerType())\n",
    "\n",
    "# *************************************************************\n",
    "def character_count(string):\n",
    "    '''\n",
    "    Count number of characters in the tweet\n",
    "    '''\n",
    "    if string:\n",
    "        return len(string)\n",
    "    else: \n",
    "        return 0\n",
    "character_count_udf = func.udf(lambda x: character_count(x), IntegerType())\n",
    "\n",
    "# *************************************************************\n",
    "def extract_date_info(string, info_type):\n",
    "    '''\n",
    "    IN WORK\n",
    "    Extract date info\n",
    "    '''\n",
    "    date = datetime.datetime.strptime(string, '%m/%d/%Y %H:%M')\n",
    "    \n",
    "    if info_type == 'minute':\n",
    "        info = date.minute\n",
    "    elif info_type == 'hour':\n",
    "        info = date.hour\n",
    "    elif info_type == 'day':\n",
    "        info = date.day\n",
    "    elif info_type == 'month':\n",
    "        info = date.month\n",
    "    elif info_type == 'year':\n",
    "        info = date.year    \n",
    "    return info\n",
    "extract_date_info_udf = func.udf(lambda x,y: extract_date_info(x,y), IntegerType())\n",
    "\n",
    "# *************************************************************\n",
    "\n",
    "def assignLabel(account_category):\n",
    "    '''\n",
    "        Assigns 1 - troll, or 0 - not-troll as a label to the tweet.\n",
    "    '''\n",
    "    if account_category in (\"RightTroll\", \"LeftTroll\" , \"Fearmonger\"):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0    \n",
    "# test = assignLabel(\"Commercial\")\n",
    "# print(test)\n",
    "assignLabel_udf = func.udf(assignLabel, IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe witih all columns from feature extraction\n",
    "df_enriched = df.withColumn(\"curated_content\", convert_emojii_UDF(col(\"content\"))) \\\n",
    "                .withColumn(\"tco1_step1_domain\", extract_domain_information_UDF(col(\"tco1_step1\"))) \\\n",
    "                .withColumn(\"tco2_step1_domain\", extract_domain_information_UDF(col(\"tco2_step1\"))) \\\n",
    "                .withColumn(\"tco3_step1_domain\", extract_domain_information_UDF(col(\"tco3_step1\"))) \\\n",
    "                .withColumn(\"handles\", extract_handles_UDF(col(\"content\"))) \\\n",
    "                .withColumn('emoji_count', count_emoji_udf(col('content'))) \\\n",
    "                .withColumn('emoji_text', extract_emoji_udf(col('content'))) \\\n",
    "                .withColumn('word_count', word_count_udf(col('content'))) \\\n",
    "                .withColumn('char_count', character_count_udf(col('content'))) \\\n",
    "                .withColumn('urls', extract_urls_udf(col('content'))) \\\n",
    "                .withColumn('url_count', url_count_udf(col('content'))) \\\n",
    "                .withColumn('url_hosts', extract_urls_redirect_base_udf(col('tco1_step1'), col('tco2_step1'), col('tco3_step1'))) \\\n",
    "                .withColumn('label',assignLabel_udf(df['account_category']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+-----------------+------------------+--------------------+------------------+------------------+------------------+\n",
      "|summary|  publish_date|        following|         followers|         emoji_count|        word_count|        char_count|         url_count|\n",
      "+-------+--------------+-----------------+------------------+--------------------+------------------+------------------+------------------+\n",
      "|  count|       2096049|          2096049|           2096049|             2096049|           2096049|           2096049|           2096049|\n",
      "|   mean|          null|4241.562128557109| 7146.343682328037|0.048428257163835385|13.181881721276554| 99.08216840350583| 0.863807096112734|\n",
      "| stddev|          null| 6231.62797308032|11624.817215193514|  0.5200131955777119| 5.479144213764923|35.353085319885935|0.7578290022168858|\n",
      "|    min|1/1/2013 16:16|                0|                 0|                   0|                 0|                 0|                 0|\n",
      "|    25%|          null|              606|               636|                   0|                10|                72|                 0|\n",
      "|    50%|          null|             2035|              1859|                   0|                12|               100|                 1|\n",
      "|    75%|          null|             5738|             12304|                   0|                16|               129|                 1|\n",
      "|    max| 9/9/2017 7:26|            76210|            206686|                 118|               123|               936|                 6|\n",
      "+-------+--------------+-----------------+------------------+--------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_enriched.select('publish_date','following','followers', 'emoji_count','word_count', 'char_count', 'url_count').summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_enriched\n",
    "#df_enriched.write.parquet('troll_tweet_full.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 2: Data Splitting / Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, testing = df.randomSplit([0.6, 0.4], seed=314)\n",
    "training_len = training.count()\n",
    "training_troll_len = training.filter(training['label']==1).count()\n",
    "testing_len = testing.count()\n",
    "testing_troll_len = testing.filter(testing['label']==1).count()\n",
    "\n",
    "print(\"Training count: {} || Training Troll Count and Ratio: {}, {}\".format(training_len, training_troll_len, training_troll_len/training_len))\n",
    "print(\"Testing count : {} || Testing Troll Count and Ratio: {}, {}\".format(testing_len, testing_troll_len, testing_troll_len/testing_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.write.parquet('tyler_training.parquet')\n",
    "testing.write.parquet('tyler_testing.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = spark.read.parquet('tyler_training.parquet')\n",
    "testing = spark.read.parquet('tyler_testing.parquet')\n",
    "training = training.dropna(subset = 'content')\n",
    "testing = testing.dropna(subset = 'content')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 3: Exploratory Data Analysis and Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique counts of categorical columns:\n",
    "from pyspark.sql.functions import desc\n",
    "df.groupBy(\"region\").count().sort(desc('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"post_type\").count().sort(desc('count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"account_category\").count().sort(desc('count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Visualization\n",
    "Create some sample graphics that detail features within the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "visuals = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if visuals:\n",
    "\n",
    "    # Visualize Follwers per account per post\n",
    "    bins, counts = df.select('followers').rdd.flatMap(lambda x: x).histogram([0,10, 100, 1000, 10000, 1000000, 10000000])\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(2, 1, 1)\n",
    "    ax.hist(bins[:-1], bins=bins, weights=counts)\n",
    "    ax.set_xscale('log')\n",
    "    plt.xlabel('Number of Followers')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Histogram of Followers per Account (log scale)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if visuals:\n",
    "    # Visualize Word Count\n",
    "    bins, counts = df.select('word_count').rdd.flatMap(lambda x:x).histogram(10)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(2, 1, 1)\n",
    "    ax.hist(bins[:-1], bins=bins, weights=counts)\n",
    "    plt.xlabel('Number of Words in Tweet')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Histogram of Words in Tweet')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if visuals:\n",
    "    # Visualize Character Count\n",
    "    bins, counts = df.select('char_count').rdd.flatMap(lambda x:x).histogram(10)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(2, 1, 1)\n",
    "    ax.hist(bins[:-1], bins=bins, weights=counts)\n",
    "    plt.xlabel('Number of Characters in Tweet')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Histogram of Characters in Tweet')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if visuals:\n",
    "    # Visualize Number of URLs\n",
    "    bins, counts = df.select('url_count').rdd.flatMap(lambda x:x).histogram(10)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(2, 1, 1)\n",
    "    ax.hist(bins[:-1], bins=bins, weights=counts)\n",
    "    plt.xlabel('Number of URLs in Tweet')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Histogram of URLs in Tweet')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if visuals:\n",
    "    # Visualize Number of Emojis\n",
    "    bins, counts = df.select('emoji_count').rdd.flatMap(lambda x:x).histogram(10)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(2, 1, 1)\n",
    "    ax.hist(bins[:-1], bins=bins, weights=counts)\n",
    "    plt.xlabel('Number of Emojis in Tweet')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Histogram of Emojis in Tweet')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (366 kB)\n",
      "\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 366 kB 17.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from wordcloud) (8.2.0)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from wordcloud) (3.3.2)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /opt/conda/lib/python3.7/site-packages (from wordcloud) (1.19.5)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in /opt/conda/lib/python3.7/site-packages (from matplotlib->wordcloud) (2021.5.30)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->wordcloud) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /opt/conda/lib/python3.7/site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.15.0)\n",
      "Installing collected packages: wordcloud\n",
      "\u001b[33m  WARNING: The script wordcloud_cli is installed in '/home/tje6gt/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed wordcloud-1.8.1\n"
     ]
    }
   ],
   "source": [
    "#! pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.types import StringType, ArrayType, IntegerType\n",
    "\n",
    "def clean_tokens(string_array):\n",
    "    import re\n",
    "    cleaned_words = ['']\n",
    "    for word in string_array:\n",
    "        #if word[0] == '#':\n",
    "        #    clean = re.sub('[^\\P{P}#]+', \"\", word)\n",
    "        #else:\n",
    "        if 'http' in word:\n",
    "            continue\n",
    "        else:\n",
    "            clean = re.sub(\"[^A-Za-z]+\", \"\", word)\n",
    "        \n",
    "        cleaned_words.append(clean)\n",
    "\n",
    "    \n",
    "    cleaned_words = [word for word in cleaned_words if word]\n",
    "    return cleaned_words\n",
    "clean_tokens_udf = func.udf(lambda x: clean_tokens(x), ArrayType(StringType(), True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_token_words.select('cleaned_words').show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "data_full = training.union(testing)\n",
    "\n",
    "tok_content = Tokenizer(inputCol=\"content\", outputCol=\"words\")\n",
    "token_words = tok_content.transform(data_full)\n",
    "clean_token_words = token_words.withColumn(\"cleaned_words\", clean_tokens_udf(col(\"words\")))\n",
    "\n",
    "remover_content = StopWordsRemover(inputCol=\"cleaned_words\", outputCol=\"cleaned_words_filtered\")\n",
    "no_stop_words = remover_content.transform(clean_token_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "troll_words = no_stop_words.filter(no_stop_words['label'] == 1)\n",
    "not_troll_words = no_stop_words.filter(no_stop_words['label'] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "troll_only_words = troll_words.select('cleaned_words_filtered')\n",
    "troll_only_words.show(5, truncate = False)\n",
    "troll_words_final = troll_only_words.rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_troll_only_words = not_troll_words.select('cleaned_words_filtered')\n",
    "not_troll_only_words.show(5, truncate = False)\n",
    "not_troll_words_final = not_troll_only_words.rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_troll_words = ' '.join(sent for sent in [' '.join(word for word in tweet) for tweet in troll_words_final])\n",
    "all_not_troll_words = ' '.join(sent for sent in [' '.join(word for word in tweet) for tweet in not_troll_words_final])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "troll_cloud = WordCloud(max_words = 250, background_color = 'white').generate(all_troll_words)\n",
    "not_troll_cloud = WordCloud(max_words = 250, background_color = 'white').generate(all_not_troll_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(troll_cloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(not_troll_cloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 4: Model Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import context manager: SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,lit\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel, Transformer\n",
    "from pyspark.ml.feature import *  \n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n",
    "\n",
    "from pyspark import keyword_only\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create session with custom app name, grab context\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local\") \\\n",
    "        .appName(\"team1_sp22_final_project\") \\\n",
    "        .config(\"spark.executor.memory\", '150g') \\\n",
    "        .config('spark.executor.cores', '20') \\\n",
    "        .config('spark.cores.max', '20') \\\n",
    "        .config(\"spark.driver.memory\",'150g') \\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = spark.read.parquet('tyler_training.parquet')\n",
    "testing = spark.read.parquet('tyler_testing.parquet')\n",
    "training = training.dropna(subset = 'content')\n",
    "testing = testing.dropna(subset = 'content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_evaluation_metrics(prediction):\n",
    "    evaluator = BinaryClassificationEvaluator(rawPredictionCol = 'prediction', labelCol = 'label', metricName = 'areaUnderROC')\n",
    "    auROC = evaluator.evaluate(prediction)\n",
    "    print(\"AUROC: {}\".format(auROC))\n",
    "    \n",
    "    predictionsRdd = prediction.select(\"prediction\",\"label\").rdd\n",
    "    predictionsRdd = predictionsRdd.map(lambda p: (float(p.label), (float(p.prediction))))\n",
    "    metrics = MulticlassMetrics(predictionsRdd)\n",
    "    print(f'Accuracy with MulticlassMetrics is {metrics.accuracy}')\n",
    "    print(f'Precision with MulticlassMetrics is {metrics.precision}')\n",
    "    print(f'Recall with MulticlassMetrics is {metrics.recall}')\n",
    "    print(f'F1 Score with MulticlassMetrics is {metrics.fMeasure}')\n",
    "\n",
    "    print(metrics.confusionMatrix().toArray())\n",
    "    \n",
    "def cross_val_train_test(cv_model, training, testing):\n",
    "    t0 = time.time()\n",
    "    cvModel = crossval.setParallelism(20).fit(training) # train 20 models in parallel\n",
    "    print(\"train time:\", time.time() - t0)\n",
    "    print('-'*30)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    prediction = cvModel.transform(testing)\n",
    "    print(\"test time:\", time.time() - t0)\n",
    "    print('-'*30)\n",
    "    \n",
    "    return cvModel, prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Subset 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple Column Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cols = ['url_count', 'char_count', 'word_count', 'emoji_count', 'following', 'followers']\n",
    "\n",
    "va = VectorAssembler(inputCols = simple_cols, outputCol = 'features')\n",
    "lr = LogisticRegression(labelCol = 'label', featuresCol = 'features')\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr.maxIter, [5, 10]) \\\n",
    "    .build()\n",
    "\n",
    "simple_cols_pipeline = Pipeline(stages = [va, lr])\n",
    "\n",
    "crossval = CrossValidator(estimator = simple_cols_pipeline,\n",
    "                          estimatorParamMaps = paramGrid,\n",
    "                          evaluator = BinaryClassificationEvaluator(),\n",
    "                          numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_simple_cvModel, lr_simple_prediction = cross_val_train_test(crossval, training, testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_evaluation_metrics(lr_simple_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_simple_cvModel.save('lr_small_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Content Columns without URL, Text Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenCleaner(Transformer):\n",
    "    \"\"\"\n",
    "    A custom Transformer\n",
    "    \"\"\"\n",
    "    inputCol = Param(Params._dummy(), \"inputCol\", \"input column name.\", typeConverter=TypeConverters.toString)\n",
    "    outputCol = Param(Params._dummy(), \"outputCol\", \"output column name.\", typeConverter=TypeConverters.toString)\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol: str = \"input\", outputCol: str = \"output\"):\n",
    "        super(TokenCleaner, self).__init__()\n",
    "        self._setDefault(inputCol=None, outputCol=None)\n",
    "        kwargs = self._input_kwargs\n",
    "        self.set_params(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def set_params(self, inputCol: str = \"input\", outputCol: str = \"output\"):\n",
    "        kwargs = self._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "\n",
    "    def get_input_col(self):\n",
    "        return self.getOrDefault(self.inputCol)\n",
    "\n",
    "    def get_output_col(self):\n",
    "        return self.getOrDefault(self.outputCol)\n",
    "\n",
    "    def _transform(self, df: DataFrame):\n",
    "        inputCol = self.get_input_col()\n",
    "        outputCol = self.get_output_col()\n",
    "\n",
    "        transform_udf = func.udf(lambda x: clean_tokens(x), ArrayType(StringType(), True))\n",
    "\n",
    "        return df.withColumn(outputCol, transform_udf(inputCol))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_content = Tokenizer(inputCol=\"content\", outputCol=\"words\")\n",
    "\n",
    "cleaned_token = TokenCleaner(inputCol = 'words', outputCol = 'cleaned_words')\n",
    "\n",
    "remover_content = StopWordsRemover(inputCol=\"cleaned_words\", outputCol=\"words_filtered\")\n",
    "htf_content = HashingTF(inputCol=\"words_filtered\", outputCol=\"content_htf\")  \n",
    "\n",
    "va = VectorAssembler(inputCols=[\"content_htf\"], outputCol=\"features\")\n",
    "lr = LogisticRegression(labelCol = 'label', featuresCol = 'features')\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(htf_content.numFeatures, [10]) \\\n",
    "    .addGrid(lr.maxIter, [5]) \\\n",
    "    .addGrid(lr.regParam, [0.1]) \\\n",
    "    .build()\n",
    "\n",
    "content_cols_pipeline = Pipeline(stages = [\n",
    "                                          tok_content,\n",
    "                                          cleaned_token,\n",
    "                                          remover_content,\n",
    "                                          htf_content,\n",
    "                                          va,\n",
    "                                          lr\n",
    "                                         ])\n",
    "\n",
    "crossval = CrossValidator(estimator = content_cols_pipeline,\n",
    "                          estimatorParamMaps = paramGrid,\n",
    "                          evaluator = BinaryClassificationEvaluator(),\n",
    "                          numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_words_cvModel, lr_words_prediction = cross_val_train_test(crossval, training, testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_evaluation_metrics(lr_words_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Content Columns without URL, Add Simple Cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "va = VectorAssembler(inputCols=[\"content_htf\", 'url_count', 'char_count', 'word_count', 'emoji_count', 'following', 'followers'], outputCol=\"features\")\n",
    "lr = LogisticRegression(labelCol = 'label', featuresCol = 'features')\n",
    "\n",
    "#paramGrid = ParamGridBuilder() \\\n",
    "#    .addGrid(htf_content.numFeatures, [10, 50]) \\\n",
    "#    .addGrid(lr.maxIter, [10]) \\\n",
    "#    .addGrid(lr.regParam, [0.01]) \\\n",
    "#    .build()\n",
    "\n",
    "#paramGrid = ParamGridBuilder() \\\n",
    "#    .addGrid(htf_content.numFeatures, [10]) \\\n",
    "#    .addGrid(lr.maxIter, [5]) \\\n",
    "#    .addGrid(lr.regParam, [0.1]) \\\n",
    "#    .build()\n",
    "\n",
    "\n",
    "content_simple_cols_pipeline = Pipeline(stages = [\n",
    "                                          tok_content,\n",
    "                                          cleaned_token,\n",
    "                                          remover_content,\n",
    "                                          htf_content,\n",
    "                                          va,\n",
    "                                          lr\n",
    "                                         ])\n",
    "\n",
    "crossval = CrossValidator(estimator = content_simple_cols_pipeline,\n",
    "                          estimatorParamMaps = paramGrid,\n",
    "                          evaluator = BinaryClassificationEvaluator(),\n",
    "                          numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_complex_cvModel, lr_complex_prediction = cross_val_train_test(crossval, training, testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_evaluation_metrics(lr_copmlex_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Subset 2: Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Simple Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "va = VectorAssembler(inputCols = simple_cols, outputCol = 'features')\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label')\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.maxDepth, [3, 5, 10]) \\\n",
    "    .addGrid(rf.numTrees, [5, 10, 20]) \\\n",
    "    .build()\n",
    "\n",
    "rf_simple_cols_pipeline = Pipeline(stages = [va, rf])\n",
    "\n",
    "crossval = CrossValidator(estimator = rf_simple_cols_pipeline,\n",
    "                          estimatorParamMaps = paramGrid,\n",
    "                          evaluator = BinaryClassificationEvaluator(),\n",
    "                          numFolds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_simple_cvModel, rf_simple_prediction = cross_val_train_test(crossval, training, testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_evaluation_metrics(rf_simple_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_simple_cvModel.save('rf_small_model_4_8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Content and Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "va = VectorAssembler(inputCols=[\"content_htf\", 'url_count', 'char_count', 'word_count', 'emoji_count', 'following', 'followers'], outputCol=\"features\")\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label')\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(htf_content.numFeatures, [10, 50]) \\\n",
    "    .addGrid(rf.maxDepth, [5, 10]) \\\n",
    "    .addGrid(rf.numTrees, [20]) \\\n",
    "    .build()\n",
    "\n",
    "content_cols_pipeline = Pipeline(stages = [\n",
    "                                          tok_content,\n",
    "                                          cleaned_token,\n",
    "                                          remover_content,\n",
    "                                          htf_content,\n",
    "                                          va,\n",
    "                                          rf\n",
    "                                         ])\n",
    "\n",
    "crossval = CrossValidator(estimator = content_cols_pipeline,\n",
    "                          estimatorParamMaps = paramGrid,\n",
    "                          evaluator = BinaryClassificationEvaluator(),\n",
    "                          numFolds=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_complex_cvModel, rf_complex_prediction = cross_val_train_test(crossval, training, testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_evaluation_metrics(rf_complex_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECTION 5: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
