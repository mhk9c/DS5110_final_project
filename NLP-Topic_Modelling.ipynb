{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "installing package 1\n",
      "installing package 2\n",
      "installing package 3\n",
      "installing package 4\n",
      "installing package 5\n",
      "installing package 6\n",
      "installing package 7\n",
      "Done Installing packages\n"
     ]
    }
   ],
   "source": [
    "from utils_nlp import Tools\n",
    "tools = Tools('mhk9c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "import sparknlp\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load and Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading from /project/ds5559/team1_sp22/data//russian-troll-tweets-enriched.\n"
     ]
    }
   ],
   "source": [
    "df = tools.load_data(spark, \"russian-troll-tweets-enriched\")\n",
    "df = df.withColumn(\"publish_date_timestamp\",F.to_timestamp(F.col(\"publish_date\"),\"M/d/yyyy H:mm\"))\n",
    "df = df.withColumn(\"publish_date_date\",F.to_date(F.col(\"publish_date_timestamp\")))\n",
    "df = df.withColumn(\"publish_hour\", F.hour(F.col(\"publish_date_timestamp\")))\n",
    "\n",
    "df = df.filter((df[\"publish_date_date\"] >= F.lit(\"2014-10-14\")) & (df[\"publish_date_date\"] <= F.lit(\"2017-12-14\"))) \n",
    "df_troll = df.filter(df['label']==1)\n",
    "df_nontroll = df.filter(df['label']==0)\n",
    "\n",
    "df_wikileaks = df.filter((df[\"publish_date_date\"] >= F.lit(\"2016-10-05\")) & (df[\"publish_date_date\"] <= F.lit(\"2016-10-07\"))) \n",
    "df_wikileaks_troll = df.filter(df['label']==1)\n",
    "\n",
    "df_utr = df.filter((df[\"publish_date_date\"] >= F.lit(\"2017-08-01\")) & (df[\"publish_date_date\"] <= F.lit(\"2017-08-31\"))) \n",
    "df_utr_troll = df.filter(df['label']==1)\n",
    "\n",
    "\n",
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark NLP requires the input dataframe or column to be converted to document. \n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"content\") \\\n",
    "    .setOutputCol(\"document\") \\\n",
    "    .setCleanupMode(\"shrink\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sentence to tokens(array)\n",
    "tokenizer = Tokenizer() \\\n",
    "  .setInputCols([\"document\"]) \\\n",
    "  .setOutputCol(\"token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean unwanted characters and garbage\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "stopwords_cleaner = StopWordsCleaner()\\\n",
    "      .setInputCols(\"normalized\")\\\n",
    "      .setOutputCol(\"cleanTokens\")\\\n",
    "      .setCaseSensitive(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem the words to bring them to the root form.\n",
    "stemmer = Stemmer() \\\n",
    "    .setInputCols([\"cleanTokens\"]) \\\n",
    "    .setOutputCol(\"stem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"stem\"]) \\\n",
    "    .setOutputCols([\"tokens\"]) \\\n",
    "    .setOutputAsArray(True) \\\n",
    "    .setCleanAnnotations(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build a ml pipeline so that each phase can be executed in sequence. This pipeline can also be used to test the model. \n",
    "nlp_pipeline = Pipeline(\n",
    "    stages=[document_assembler, \n",
    "            tokenizer,\n",
    "            normalizer,\n",
    "            stopwords_cleaner, \n",
    "            stemmer, \n",
    "            finisher])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(_df):\n",
    "    # train the pipeline\n",
    "    nlp_model = nlp_pipeline.fit(_df)\n",
    "    \n",
    "    # apply the pipeline to transform dataframe.\n",
    "    processed_df  = nlp_model.transform(_df)\n",
    "    \n",
    "    # tokens_df = processed_df.select('publish_date','tokens').limit(10000)\n",
    "    tokens_df = processed_df.select('publish_date','tokens')\n",
    "    \n",
    "    cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"features\", vocabSize=500, minDF=3.0)\n",
    "    \n",
    "    # train the model\n",
    "    cv_model = cv.fit(tokens_df)\n",
    "    \n",
    "    # transform the data. Output column name will be features.\n",
    "    vectorized_tokens = cv_model.transform(tokens_df)\n",
    "    \n",
    "    num_topics = 3\n",
    "    lda = LDA(k=num_topics, maxIter=10)\n",
    "    model = lda.fit(vectorized_tokens)\n",
    "    ll = model.logLikelihood(vectorized_tokens)\n",
    "    lp = model.logPerplexity(vectorized_tokens)\n",
    "    print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "    print(\"The upper bound on perplexity: \" + str(lp))\n",
    "    \n",
    "    # extract vocabulary from CountVectorizer\n",
    "    vocab = cv_model.vocabulary\n",
    "    topics = model.describeTopics()   \n",
    "    topics_rdd = topics.rdd\n",
    "    topics_words = topics_rdd\\\n",
    "           .map(lambda row: row['termIndices'])\\\n",
    "           .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
    "           .collect()\n",
    "    for idx, topic in enumerate(topics_words):\n",
    "        print(\"topic: {}\".format(idx))\n",
    "        print(\"*\"*25)\n",
    "        for word in topic:\n",
    "           print(word)\n",
    "        print(\"*\"*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lower bound on the log likelihood of the entire corpus: -26368732.106566556\n",
      "The upper bound on perplexity: 6.066693609549476\n",
      "topic: 0\n",
      "*************************\n",
      "peopl\n",
      "black\n",
      "u\n",
      "im\n",
      "dont\n",
      "like\n",
      "get\n",
      "know\n",
      "white\n",
      "make\n",
      "*************************\n",
      "topic: 1\n",
      "*************************\n",
      "trump\n",
      "break\n",
      "hillari\n",
      "new\n",
      "look\n",
      "clinton\n",
      "u\n",
      "video\n",
      "presid\n",
      "cnn\n",
      "*************************\n",
      "topic: 2\n",
      "*************************\n",
      "rt\n",
      "new\n",
      "amp\n",
      "obama\n",
      "get\n",
      "kill\n",
      "american\n",
      "realdonaldtrump\n",
      "man\n",
      "trump\n",
      "*************************\n"
     ]
    }
   ],
   "source": [
    "run_pipeline(df_wikileaks_troll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
