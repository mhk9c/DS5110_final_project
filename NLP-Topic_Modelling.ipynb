{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "installing package 1\n",
      "installing package 2\n",
      "installing package 3\n",
      "installing package 4\n",
      "installing package 5\n",
      "Done Installing packages\n"
     ]
    }
   ],
   "source": [
    "from utils_nlp import Tools\n",
    "tools = Tools('mhk9c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "import sparknlp\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.clustering import LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sparknlp\n",
    "spark = sparknlp.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"Spark NLP\")\\\n",
    "#     .config(\"spark.driver.memory\",\"8G\")\\\n",
    "#     .config(\"spark.driver.maxResultSize\", \"2G\") \\\n",
    "#     .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.4.5\")\\\n",
    "#     .config(\"spark.kryoserializer.buffer.max\", \"1000M\")\\\n",
    "#     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading from /project/ds5559/team1_sp22/data//russian-troll-tweets-enriched.\n",
      "root\n",
      " |-- external_author_id: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- publish_date: string (nullable = true)\n",
      " |-- harvested_date: string (nullable = true)\n",
      " |-- following: integer (nullable = true)\n",
      " |-- followers: integer (nullable = true)\n",
      " |-- updates: integer (nullable = true)\n",
      " |-- post_type: string (nullable = true)\n",
      " |-- account_type: string (nullable = true)\n",
      " |-- retweet: integer (nullable = true)\n",
      " |-- account_category: string (nullable = true)\n",
      " |-- new_june_2018: integer (nullable = true)\n",
      " |-- alt_external_id: string (nullable = true)\n",
      " |-- tweet_id: string (nullable = true)\n",
      " |-- article_url: string (nullable = true)\n",
      " |-- tco1_step1: string (nullable = true)\n",
      " |-- tco2_step1: string (nullable = true)\n",
      " |-- tco3_step1: string (nullable = true)\n",
      " |-- curated_content: string (nullable = true)\n",
      " |-- tco1_step1_domain: string (nullable = true)\n",
      " |-- tco2_step1_domain: string (nullable = true)\n",
      " |-- tco3_step1_domain: string (nullable = true)\n",
      " |-- handles: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- emoji_count: integer (nullable = true)\n",
      " |-- emoji_text: string (nullable = true)\n",
      " |-- word_count: integer (nullable = true)\n",
      " |-- char_count: integer (nullable = true)\n",
      " |-- urls: string (nullable = true)\n",
      " |-- url_count: integer (nullable = true)\n",
      " |-- hashes: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- url_hosts: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_df = tools.load_data(spark, \"russian-troll-tweets-enriched\")\n",
    "df = _df.filter(_df['label']==1)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1120229"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark NLP requires the input dataframe or column to be converted to document. \n",
    "document_assembler = DocumentAssembler() \\\n",
    "    .setInputCol(\"content\") \\\n",
    "    .setOutputCol(\"document\") \\\n",
    "    .setCleanupMode(\"shrink\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split sentence to tokens(array)\n",
    "tokenizer = Tokenizer() \\\n",
    "  .setInputCols([\"document\"]) \\\n",
    "  .setOutputCol(\"token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean unwanted characters and garbage\n",
    "normalizer = Normalizer() \\\n",
    "    .setInputCols([\"token\"]) \\\n",
    "    .setOutputCol(\"normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "stopwords_cleaner = StopWordsCleaner()\\\n",
    "      .setInputCols(\"normalized\")\\\n",
    "      .setOutputCol(\"cleanTokens\")\\\n",
    "      .setCaseSensitive(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem the words to bring them to the root form.\n",
    "stemmer = Stemmer() \\\n",
    "    .setInputCols([\"cleanTokens\"]) \\\n",
    "    .setOutputCol(\"stem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "finisher = Finisher() \\\n",
    "    .setInputCols([\"stem\"]) \\\n",
    "    .setOutputCols([\"tokens\"]) \\\n",
    "    .setOutputAsArray(True) \\\n",
    "    .setCleanAnnotations(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build a ml pipeline so that each phase can be executed in sequence. This pipeline can also be used to test the model. \n",
    "nlp_pipeline = Pipeline(\n",
    "    stages=[document_assembler, \n",
    "            tokenizer,\n",
    "            normalizer,\n",
    "            stopwords_cleaner, \n",
    "            stemmer, \n",
    "            finisher])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the pipeline\n",
    "nlp_model = nlp_pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the pipeline to transform dataframe.\n",
    "processed_df  = nlp_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+\n",
      "|   publish_date|              tokens|\n",
      "+---------------+--------------------+\n",
      "|11/11/2015 2:41|[jebbush, dont, l...|\n",
      "|11/11/2015 2:42|[your, talk, immi...|\n",
      "|11/11/2015 4:22|[gopstop, gopdeb,...|\n",
      "|11/11/2015 4:53|[adammathisss, sl...|\n",
      "|11/11/2015 5:08|[teaparti, gopsto...|\n",
      "|11/11/2015 5:09|[nowthisnew, gopd...|\n",
      "|11/11/2015 5:14|[republican, raci...|\n",
      "|11/11/2015 5:26|[imposs, watch, f...|\n",
      "|12/7/2015 18:58|[excit, new, on, ...|\n",
      "|12/7/2015 18:59|[reason, anyon, f...|\n",
      "|12/7/2015 18:59|[watch, on, amaz,...|\n",
      "|12/7/2015 18:59|[todai, join, u, ...|\n",
      "|12/7/2015 18:59|[tomorrow, night,...|\n",
      "|12/7/2015 18:59|[amaz, what, amaz...|\n",
      "|12/7/2015 18:59|[mi, amiga, evalo...|\n",
      "|12/7/2015 18:59|[ooher, imaceleb,...|\n",
      "|12/7/2015 19:00|[true, httpstcowh...|\n",
      "| 9/19/2015 8:23|[firearm, crimin,...|\n",
      "| 9/19/2015 8:24|[second, amend, c...|\n",
      "| 9/19/2015 8:24|[everyon, want, b...|\n",
      "+---------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tokens_df = processed_df.select('publish_date','tokens').limit(10000)\n",
    "tokens_df = processed_df.select('publish_date','tokens')\n",
    "tokens_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"features\", vocabSize=500, minDF=3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "cv_model = cv.fit(tokens_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the data. Output column name will be features.\n",
    "vectorized_tokens = cv_model.transform(tokens_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lower bound on the log likelihood of the entire corpus: -27599930.324379705\n",
      "The upper bound on perplexity: 6.290721487034518\n"
     ]
    }
   ],
   "source": [
    "num_topics = 10\n",
    "lda = LDA(k=num_topics, maxIter=10)\n",
    "model = lda.fit(vectorized_tokens)\n",
    "ll = model.logLikelihood(vectorized_tokens)\n",
    "lp = model.logPerplexity(vectorized_tokens)\n",
    "print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n",
    "print(\"The upper bound on perplexity: \" + str(lp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: 0\n",
      "*************************\n",
      "polic\n",
      "new\n",
      "kill\n",
      "state\n",
      "man\n",
      "muslim\n",
      "cop\n",
      "offic\n",
      "music\n",
      "playe\n",
      "*************************\n",
      "topic: 1\n",
      "*************************\n",
      "get\n",
      "hillari\n",
      "time\n",
      "u\n",
      "dont\n",
      "know\n",
      "clinton\n",
      "rt\n",
      "let\n",
      "year\n",
      "*************************\n",
      "topic: 2\n",
      "*************************\n",
      "rt\n",
      "peopl\n",
      "media\n",
      "presid\n",
      "realdonaldtrump\n",
      "first\n",
      "trump\n",
      "on\n",
      "love\n",
      "us\n",
      "*************************\n",
      "topic: 3\n",
      "*************************\n",
      "black\n",
      "via\n",
      "trump\n",
      "work\n",
      "world\n",
      "war\n",
      "anoth\n",
      "chang\n",
      "protest\n",
      "peopl\n",
      "*************************\n",
      "topic: 4\n",
      "*************************\n",
      "obama\n",
      "trump\n",
      "think\n",
      "right\n",
      "america\n",
      "american\n",
      "great\n",
      "that\n",
      "patriot\n",
      "back\n",
      "*************************\n",
      "topic: 5\n",
      "*************************\n",
      "rt\n",
      "cnn\n",
      "break\n",
      "maga\n",
      "trump\n",
      "new\n",
      "watch\n",
      "tcot\n",
      "fakenew\n",
      "potu\n",
      "*************************\n",
      "topic: 6\n",
      "*************************\n",
      "video\n",
      "white\n",
      "new\n",
      "im\n",
      "hous\n",
      "rt\n",
      "citi\n",
      "pleas\n",
      "top\n",
      "amp\n",
      "*************************\n",
      "topic: 7\n",
      "*************************\n",
      "want\n",
      "got\n",
      "see\n",
      "blacklivesmatt\n",
      "real\n",
      "well\n",
      "right\n",
      "rt\n",
      "go\n",
      "never\n",
      "*************************\n",
      "topic: 8\n",
      "*************************\n",
      "trump\n",
      "rt\n",
      "amp\n",
      "call\n",
      "ne\n",
      "support\n",
      "good\n",
      "donald\n",
      "break\n",
      "new\n",
      "*************************\n",
      "topic: 9\n",
      "*************************\n",
      "like\n",
      "life\n",
      "look\n",
      "trump\n",
      "make\n",
      "u\n",
      "todai\n",
      "happen\n",
      "pjnet\n",
      "dai\n",
      "*************************\n"
     ]
    }
   ],
   "source": [
    "# extract vocabulary from CountVectorizer\n",
    "vocab = cv_model.vocabulary\n",
    "topics = model.describeTopics()   \n",
    "topics_rdd = topics.rdd\n",
    "topics_words = topics_rdd\\\n",
    "       .map(lambda row: row['termIndices'])\\\n",
    "       .map(lambda idx_list: [vocab[idx] for idx in idx_list])\\\n",
    "       .collect()\n",
    "for idx, topic in enumerate(topics_words):\n",
    "    print(\"topic: {}\".format(idx))\n",
    "    print(\"*\"*25)\n",
    "    for word in topic:\n",
    "       print(word)\n",
    "    print(\"*\"*25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
