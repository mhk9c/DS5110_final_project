{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The type of model (e.g., logistic regression)\n",
    "* We used LogisticRegression for our first attempt.\n",
    "### Best hyperparameters used\n",
    "* We did not tune LogisticRegresssion hyperparameters. \n",
    "* As we test different models we will tune the hyperparameters.\n",
    "### Size of the saved model\n",
    "* The final size of the model is 732K.\n",
    "### Performance metrics\n",
    "* Accuracy with MulticlassMetrics is 81.9%\n",
    "\n",
    "|  | Predicted Yes | Predicted No |\n",
    "| --- | --- | --- |\n",
    "| Actual Yes | 317572 | 78047 |\n",
    "| Actual No | 73909 | 370115 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,lit\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.mllib.stat import Statistics\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import *  \n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "from utils import Tools\n",
    "tools = Tools('mhk9c')\n",
    "spark = tools.spark\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Create features\n",
    "import emoji\n",
    "import re\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading from /project/ds5559/team1_sp22/data//russian-troll-tweets-enriched.\n",
      "root\n",
      " |-- external_author_id: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- content: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- language: string (nullable = true)\n",
      " |-- publish_date: string (nullable = true)\n",
      " |-- harvested_date: string (nullable = true)\n",
      " |-- following: integer (nullable = true)\n",
      " |-- followers: integer (nullable = true)\n",
      " |-- updates: integer (nullable = true)\n",
      " |-- post_type: string (nullable = true)\n",
      " |-- account_type: string (nullable = true)\n",
      " |-- retweet: integer (nullable = true)\n",
      " |-- account_category: string (nullable = true)\n",
      " |-- new_june_2018: integer (nullable = true)\n",
      " |-- alt_external_id: string (nullable = true)\n",
      " |-- tweet_id: string (nullable = true)\n",
      " |-- article_url: string (nullable = true)\n",
      " |-- tco1_step1: string (nullable = true)\n",
      " |-- tco2_step1: string (nullable = true)\n",
      " |-- tco3_step1: string (nullable = true)\n",
      " |-- curated_content: string (nullable = true)\n",
      " |-- tco1_step1_domain: string (nullable = true)\n",
      " |-- tco2_step1_domain: string (nullable = true)\n",
      " |-- tco3_step1_domain: string (nullable = true)\n",
      " |-- handles: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- emoji_count: integer (nullable = true)\n",
      " |-- emoji_text: string (nullable = true)\n",
      " |-- word_count: integer (nullable = true)\n",
      " |-- char_count: integer (nullable = true)\n",
      " |-- urls: string (nullable = true)\n",
      " |-- url_count: integer (nullable = true)\n",
      " |-- url_hosts: string (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = tools.load_data(\"russian-troll-tweets-enriched\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into test and train\n",
    "training, test = df.randomSplit([0.6, 0.4], seed=314)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------------+\n",
      "|   tco1_step1_domain|count(tco1_step1_domain)|\n",
      "+--------------------+------------------------+\n",
      "|         twitter.com|                  366161|\n",
      "|                  NA|                  300003|\n",
      "|              bit.ly|                   40029|\n",
      "|              ift.tt|                   15574|\n",
      "|              zpr.io|                    6629|\n",
      "|               ow.ly|                    4507|\n",
      "|               fb.me|                    4398|\n",
      "|  EXERCISEWORKOUT.PW|                    4262|\n",
      "|    EXERCISEQUOTE.PW|                    3900|\n",
      "|            youtu.be|                    3798|\n",
      "|         youtube.com|                    3789|\n",
      "|       instagram.com|                    3279|\n",
      "|hedgeaccordingly.com|                    2460|\n",
      "|             vine.co|                    2268|\n",
      "|               ln.is|                    2001|\n",
      "|              goo.gl|                    1767|\n",
      "|              dld.bz|                    1661|\n",
      "|           1063.mobi|                    1618|\n",
      "|        ImageEra.com|                    1481|\n",
      "|           dailym.ai|                    1262|\n",
      "+--------------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test.createOrReplaceTempView(\"test\")\n",
    "sqlDF = spark.sql(\"SELECT tco1_step1_domain, count(tco1_step1_domain) FROM test GROUP BY tco1_step1_domain ORDER BY COUNT(tco1_step1_domain) DESC\")\n",
    "# sqlDF = spark.sql(\"SELECT count(content) FROM test\")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set count : 1256405, ratio of trolls : 0.534912707287857\n",
      "Test set count : 839643, ratio of trolls : 0.533753035516285\n"
     ]
    }
   ],
   "source": [
    "# Check splits\n",
    "training_count = training.count()\n",
    "training_trolls = training.filter(training['label']==1).count()\n",
    "\n",
    "test_count = test.count()\n",
    "test_trolls = test.filter(test['label']==1).count()\n",
    "\n",
    "print(f'Train set count : {training.count()}, ratio of trolls : {training_trolls/training_count}')\n",
    "print(f'Test set count : {test.count()}, ratio of trolls : {test_trolls/test_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features that we can use without any processing.\n",
    "feats = ['url_count', 'char_count', 'word_count', 'emoji_count']\n",
    "# Some thoughts:\n",
    "# url_hosts and handles are both arrays of texts. It feels like we should be able to do some kind of multi-category one-hot encoding\n",
    "# A quick fix is to explode these columns and one-hot encode each one.\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_content = Tokenizer(inputCol=\"content\", outputCol=\"words\")\n",
    "remover_content = StopWordsRemover(inputCol=\"words\", outputCol=\"words_filtered\")\n",
    "htf_content = HashingTF(inputCol=\"words_filtered\", outputCol=\"content_htf\", numFeatures=200)  \n",
    "\n",
    "tok_emoji_text = Tokenizer(inputCol=\"emoji_text\", outputCol=\"emoji_text_words\")\n",
    "remover_emoji_text = StopWordsRemover(inputCol=\"emoji_text_words\", outputCol=\"emoji_text_words_filtered\")\n",
    "htf_emoji_text = HashingTF(inputCol=\"emoji_text_words_filtered\", outputCol=\"emoji_text_htf\", numFeatures=200)  \n",
    "\n",
    "# Instead of using the pre filtered domains, we probably want to use our own, derived from the content.\n",
    "# An example of how to do this is here : https://sparkbyexamples.com/spark/spark-dataframe-withcolumn/\n",
    "\n",
    "stringIndexer_d1 = StringIndexer(inputCol=\"tco1_step1_domain\", outputCol=\"d1_Index\", handleInvalid='keep')\n",
    "ohe_d1 = OneHotEncoder(inputCol=\"d1_Index\", outputCol=\"d1_vec\") \n",
    "\n",
    "stringIndexer_d2 = StringIndexer(inputCol=\"tco2_step1_domain\", outputCol=\"d2_Index\", handleInvalid='keep')\n",
    "ohe_d2 = OneHotEncoder(inputCol=\"d2_Index\", outputCol=\"d2_vec\") \n",
    "\n",
    "stringIndexer_d3 = StringIndexer(inputCol=\"tco3_step1_domain\", outputCol=\"d3_Index\", handleInvalid='keep')\n",
    "ohe_d3 = OneHotEncoder(inputCol=\"d3_Index\", outputCol=\"d3_vec\") \n",
    "\n",
    "va = VectorAssembler(inputCols=[\"content_htf\",\"emoji_text_htf\", \"d1_vec\", \"d2_vec\", \"d3_vec\", \"url_count\", \"char_count\", \"word_count\", \"emoji_count\" ], outputCol=\"features\")  \n",
    "lr = LogisticRegression(labelCol='label', featuresCol='features', maxIter=10, regParam=0.01)\n",
    "\n",
    "# Fit the pipeline\n",
    "pipeline = Pipeline(stages=[\n",
    "                            tok_content\n",
    "                            ,remover_content\n",
    "                            ,htf_content\n",
    "                            ,tok_emoji_text\n",
    "                            ,remover_emoji_text\n",
    "                            ,htf_emoji_text\n",
    "                            ,stringIndexer_d1\n",
    "                            ,ohe_d1\n",
    "                            ,stringIndexer_d2\n",
    "                            ,ohe_d2\n",
    "                            ,stringIndexer_d3\n",
    "                            ,ohe_d3\n",
    "                            ,va\n",
    "                            ,lr])\n",
    "# model = pipeline.fit(training)\n",
    "model = pipeline.fit(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write().overwrite().save(f\"{tools.data_path}LogisticRegression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Models that we've fitted already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PipelineModel.load(f\"{tools.data_path}LogisticRegression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.types import FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsRdd = predictions.select(\"prediction\",\"label\").rdd\n",
    "predictionsRdd = predictionsRdd.map(lambda p: (float(p.label), (float(p.prediction))))\n",
    "predictionsRdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = MulticlassMetrics(predictionsRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Accuracy with MulticlassMetrics is {metrics.accuracy}')\n",
    "print(metrics.confusionMatrix().toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
