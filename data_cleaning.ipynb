{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,lit\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"team1_sp22_final_project\") \\\n",
    "    .config(\"spark.executor.memory\", '8g') \\\n",
    "    .config('spark.executor.cores', '4') \\\n",
    "    .config('spark.cores.max', '4') \\\n",
    "    .config(\"spark.driver.memory\",'8g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: demoji in /home/mhk9c/.local/lib/python3.7/site-packages (1.1.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tldextract in /home/mhk9c/.local/lib/python3.7/site-packages (3.2.0)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /home/mhk9c/.local/lib/python3.7/site-packages (from tldextract) (3.6.0)\n",
      "Requirement already satisfied: requests-file>=1.4 in /home/mhk9c/.local/lib/python3.7/site-packages (from tldextract) (1.5.1)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.7/site-packages (from tldextract) (2.10)\n",
      "Requirement already satisfied: requests>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from tldextract) (2.24.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.1.0->tldextract) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.1.0->tldextract) (2021.5.30)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.1.0->tldextract) (3.0.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from requests-file>=1.4->tldextract) (1.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel/__main__.py:12: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n"
     ]
    }
   ],
   "source": [
    "# Simple pattern to Install custom packages from Juypter.\n",
    "username = 'mhk9c'\n",
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install demoji\n",
    "!{sys.executable} -m pip install tldextract\n",
    "\n",
    "sys.path.append(f'/home/{username}/.local/lib/python3.7/site-packages/')\n",
    "\n",
    "# Then you can import them.\n",
    "import demoji \n",
    "demoji.download_codes()\n",
    "\n",
    "import tldextract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I propose that we keep all of our data, downloaded or derived, in the common folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/project/ds5559/team1_sp22/data/\"\n",
    "load_par = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df(_df, name):\n",
    "    # Check whether the specified path exists or not\n",
    "    full_path = f'{data_path}{name}'\n",
    "    print(full_path)  \n",
    "    if not os.path.exists(full_path):  \n",
    "        # Create a new directory because it does not exist \n",
    "        os.makedirs(full_path)\n",
    "        print(\"The new directory is created!\")\n",
    "    \n",
    "    _df.write.format(\"parquet\").mode(\"overwrite\").save(f\"{full_path}\")\n",
    "    os.system(f'chmod -R 777 {full_path}')\n",
    "    print(f'Saved as: {full_path}')\n",
    "\n",
    "def load_data(name):       \n",
    "    full_path = f'{data_path}/{name}'\n",
    "    _df = spark.read.parquet(full_path)        \n",
    "    print(f'Done loading from {full_path}.')\n",
    "    return _df\n",
    "    \n",
    "def create_df_from_csv(name):\n",
    "    first = True\n",
    "    for file in glob.glob(f'{data_path}/{name}/*.csv'):            \n",
    "        print(file)\n",
    "        if(first):\n",
    "            _df = spark.read.csv(file, header=True, inferSchema=True, mode=\"DROPMALFORMED\")                \n",
    "            _df = _df.withColumn(\"source_file\",lit(file))\n",
    "        else:\n",
    "            new_df = spark.read.csv(file, header=True, inferSchema=True, mode=\"DROPMALFORMED\")\n",
    "            new_df = new_df.withColumn(\"source_file\",lit(file))                \n",
    "            _df = _df.union(new_df)                        \n",
    "        first = False        \n",
    "    return _df\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done loading from /project/ds5559/team1_sp22/data//russian-troll-tweets.\n",
      "There are 2914254 tweets in this dataset\n"
     ]
    }
   ],
   "source": [
    "if(load_par):\n",
    "    df = load_data(\"russian-troll-tweets\")\n",
    "else:\n",
    "    df = create_df_from_csv(\"russian-troll-tweets-master\")\n",
    "    save_df(df, \"russian-troll-tweets\")\n",
    "    \n",
    "total_tweets = df.count()\n",
    "print(f'There are {total_tweets} tweets in this dataset')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2,096,049 english tweets in this dataset. They account for 71.924033% of the dataset.\n"
     ]
    }
   ],
   "source": [
    "sqlDF = spark.sql(\"SELECT * FROM tweets where language = 'English' \")\n",
    "english_tweets = sqlDF.count()\n",
    "print(f'There are {english_tweets:,} english tweets in this dataset. They account for {english_tweets/total_tweets:%} of the dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add some additional columns to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "import re\n",
    "b = re.compile(r\"@[a-zA-Z0-9]+\")\n",
    "\n",
    "def convert_emojii(string):    \n",
    "    try:\n",
    "        return demoji.replace_with_desc(string, \":\")\n",
    "    except:\n",
    "        return \"COULD NOT CONVERT EMOJII\"\n",
    "convert_emojii_UDF = func.udf(lambda z:convert_emojii(z),StringType())   \n",
    "# test = convert_emojii(\"üêùüêùüêù\")   \n",
    "# print(test)\n",
    "\n",
    "\n",
    "def extract_domain_information(url):\n",
    "    try:\n",
    "        if(url):\n",
    "            ext = tldextract.extract(url)\n",
    "            return ext.registered_domain\n",
    "        else:\n",
    "            return \"\"        \n",
    "    except:\n",
    "         return \"\"    \n",
    "extract_domain_information_UDF = func.udf(lambda z:extract_domain_information(z),StringType())   \n",
    "# test = extract_domain_information(\"https://rivanna-portal.hpc.virginia.edu/node/udc-ba27-18/55477/lab?\")\n",
    "# print(test)\n",
    "\n",
    "\n",
    "def extract_handles(content): \n",
    "    try:\n",
    "        if(content is not None):        \n",
    "            result = re.findall(b, content) \n",
    "            return result\n",
    "        else:\n",
    "            return []\n",
    "    except:\n",
    "        return []    \n",
    "extract_handles_UDF = func.udf(lambda z:extract_handles(z),ArrayType(StringType(), True))   \n",
    "# test = extract_handles(\"Hi @MichelleObama , remember when you praised Harvey Weinstein as 'a wonderful human being, a good friend and a powerhouse.\")\n",
    "# print(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/ds5559/team1_sp22/data/russian-troll-tweets-enriched\n",
      "Saved as: /project/ds5559/team1_sp22/data/russian-troll-tweets-enriched\n"
     ]
    }
   ],
   "source": [
    "sqlDF = sqlDF.withColumn(\"curated_content\", convert_emojii_UDF(col(\"content\"))) \\\n",
    "                .withColumn(\"tco1_step1_domain\", extract_domain_information_UDF(col(\"tco1_step1\"))) \\\n",
    "                .withColumn(\"tco2_step1_domain\", extract_domain_information_UDF(col(\"tco2_step1\"))) \\\n",
    "                .withColumn(\"tco3_step1_domain\", extract_domain_information_UDF(col(\"tco3_step1\"))) \\\n",
    "                .withColumn(\"handles\", extract_handles_UDF(col(\"content\")))\n",
    "save_df(sqlDF, \"russian-troll-tweets-enriched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDF.createOrReplaceTempView(\"english_tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_sqlDF = spark.sql(\"SELECT tco1_step1, tco2_step1, tco3_step1  FROM english_tweets LIMIT 10\")\n",
    "_sqlDF.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDF = spark.sql(\"SELECT content,source_file FROM english_tweets LIMIT 100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDF_content = spark.sql(\"SELECT content FROM tweets where language = 'English'\")\n",
    "content_RDD = sqlDF_content.rdd \n",
    "type(content_RDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_RDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_content_english.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram Word Count\n",
    "bigrams = content_RDD \\\n",
    "          .map(lambda x: x['content']) \\\n",
    "          .map(lambda x: [] if (x is None) else x.split() ) \\\n",
    "          .flatMap(lambda x: [((x[i],x[i+1]),1) for i in range(0,len(x)-1)])\\\n",
    "          .reduceByKey(lambda x,y: x+y) \\\n",
    "          .map(lambda x: (x[1],x[0])) \\\n",
    "          .sortByKey(False)\n",
    "\n",
    "bigrams.take(10)\n",
    "# Not that exciting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams.saveAsTextFile(f'{data_path}/bigrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF\n",
    "hashingTF = HashingTF()\n",
    "\n",
    "# Load documents (one per line).\n",
    "# documents = sc.textFile(\"...\").map(lambda line: line.split(\" \"))\n",
    "documents = content_RDD \\\n",
    "          .map(lambda x: x['content']) \\\n",
    "          .map(lambda x: [] if (x is None) else x.split() )         \n",
    "\n",
    "tf = hashingTF.transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.cache()\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.cache()\n",
    "idf = IDF(minDocFreq=2).fit(tf)\n",
    "tfidf = idf.transform(tf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
