{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col,lit\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: demoji in /home/mhk9c/.local/lib/python3.7/site-packages (1.1.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tldextract in /home/mhk9c/.local/lib/python3.7/site-packages (3.2.0)\n",
      "Requirement already satisfied: requests>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from tldextract) (2.24.0)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.7/site-packages (from tldextract) (2.10)\n",
      "Requirement already satisfied: filelock>=3.0.8 in /home/mhk9c/.local/lib/python3.7/site-packages (from tldextract) (3.6.0)\n",
      "Requirement already satisfied: requests-file>=1.4 in /home/mhk9c/.local/lib/python3.7/site-packages (from tldextract) (1.5.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.1.0->tldextract) (2021.5.30)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.1.0->tldextract) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.1.0->tldextract) (1.25.11)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from requests-file>=1.4->tldextract) (1.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel/__main__.py:12: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n"
     ]
    }
   ],
   "source": [
    "# Simple pattern to Install custom packages from Juypter.\n",
    "username = 'mhk9c'\n",
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install demoji\n",
    "!{sys.executable} -m pip install tldextract\n",
    "\n",
    "sys.path.append(f'/home/{username}/.local/lib/python3.7/site-packages/')\n",
    "\n",
    "# Then you can import them.\n",
    "import demoji \n",
    "demoji.download_codes()\n",
    "\n",
    "import tldextract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I propose that we keep all of our data, downloaded or derived, in the common folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/project/ds5559/team1_sp22/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/ds5559/team1_sp22/data//russian-troll-tweets-master/IRAhandle_tweets_13.csv\n",
      "/project/ds5559/team1_sp22/data//russian-troll-tweets-master/IRAhandle_tweets_9.csv\n",
      "/project/ds5559/team1_sp22/data//russian-troll-tweets-master/IRAhandle_tweets_3.csv\n",
      "/project/ds5559/team1_sp22/data//russian-troll-tweets-master/IRAhandle_tweets_12.csv\n",
      "/project/ds5559/team1_sp22/data//russian-troll-tweets-master/IRAhandle_tweets_5.csv\n",
      "/project/ds5559/team1_sp22/data//russian-troll-tweets-master/IRAhandle_tweets_2.csv\n",
      "/project/ds5559/team1_sp22/data//russian-troll-tweets-master/IRAhandle_tweets_6.csv\n",
      "/project/ds5559/team1_sp22/data//russian-troll-tweets-master/IRAhandle_tweets_11.csv\n",
      "/project/ds5559/team1_sp22/data//russian-troll-tweets-master/IRAhandle_tweets_4.csv\n",
      "/project/ds5559/team1_sp22/data//russian-troll-tweets-master/IRAhandle_tweets_7.csv\n",
      "/project/ds5559/team1_sp22/data//russian-troll-tweets-master/IRAhandle_tweets_10.csv\n",
      "/project/ds5559/team1_sp22/data//russian-troll-tweets-master/IRAhandle_tweets_8.csv\n",
      "/project/ds5559/team1_sp22/data//russian-troll-tweets-master/IRAhandle_tweets_1.csv\n",
      "There are 2946207 tweets in this dataset\n"
     ]
    }
   ],
   "source": [
    "def save_df(_df, name):\n",
    "    # Check whether the specified path exists or not\n",
    "    full_path = f'{data_path}{name}'\n",
    "    print(full_path)  \n",
    "    if not os.path.exists(full_path):  \n",
    "        # Create a new directory because it does not exist \n",
    "        os.makedirs(full_path)\n",
    "        print(\"The new directory is created!\")\n",
    "    \n",
    "    _df.write.format(\"parquet\").mode(\"overwrite\").save(f\"{full_path}\")\n",
    "    os.system(f'chmod -R 777 {full_path}')\n",
    "    print(f'Saved as: {full_path}')\n",
    "\n",
    "def load_data(name):       \n",
    "    full_path = f'{data_path}/{name}'\n",
    "    _df = spark.read.parquet(full_path)        \n",
    "    print(f'Done loading from {full_path}.')\n",
    "    return _df\n",
    "    \n",
    "def create_df_from_csv(name):\n",
    "    first = True\n",
    "    for file in glob.glob(f'{data_path}/{name}/*.csv'):            \n",
    "        print(file)\n",
    "        if(first):\n",
    "            _df = spark.read.csv(file, header=True, inferSchema=True, mode=\"DROPMALFORMED\")                \n",
    "            _df = _df.withColumn(\"source_file\",lit(file))\n",
    "        else:\n",
    "            new_df = spark.read.csv(file, header=True, inferSchema=True, mode=\"DROPMALFORMED\")\n",
    "            new_df = new_df.withColumn(\"source_file\",lit(file))                \n",
    "            _df = _df.union(new_df)                        \n",
    "        first = False        \n",
    "    return _df\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "df = create_df_from_csv(\"russian-troll-tweets-master\")\n",
    "# df = load_data(\"russian-troll-tweets\")\n",
    "# save_df(df, \"russian-troll-tweets\")\n",
    "\n",
    "total_tweets = df.count()\n",
    "print(f'There are {total_tweets} tweets in this dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/project/ds5559/team1_sp22/data/russian-troll-tweets\n",
      "Saved as: /project/ds5559/team1_sp22/data/russian-troll-tweets\n"
     ]
    }
   ],
   "source": [
    "save_df(df, \"russian-troll-tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDF = spark.sql(\"SELECT * FROM tweets where language = 'English' \")\n",
    "english_tweets = sqlDF.count()\n",
    "print(f'There are {english_tweets:,} english tweets in this dataset. They account for {english_tweets/total_tweets:%} of the dataset.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add some additional columns to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "import re\n",
    "b = re.compile(r\"@[a-zA-Z0-9]+\")\n",
    "\n",
    "def convert_emojii(string):    \n",
    "    return demoji.replace_with_desc(string, \":\")\n",
    "convert_emojii_UDF = func.udf(lambda z:convert_emojii(z),StringType())   \n",
    "# test = convert_emojii(\"üêùüêùüêù\")   \n",
    "# print(test)\n",
    "\n",
    "\n",
    "def extract_domain_information(url):\n",
    "    if(url):\n",
    "        ext = tldextract.extract(url)\n",
    "        return ext.registered_domain\n",
    "    else:\n",
    "        return \"\"\n",
    "extract_domain_information_UDF = func.udf(lambda z:extract_domain_information(z),StringType())   \n",
    "# test = extract_domain_information(\"https://rivanna-portal.hpc.virginia.edu/node/udc-ba27-18/55477/lab?\")\n",
    "# print(test)\n",
    "\n",
    "\n",
    "def extract_handles(content):     \n",
    "    if(content is not None):        \n",
    "        result = re.findall(b, content) \n",
    "        return result\n",
    "    else:\n",
    "        return []\n",
    "extract_handles_UDF = func.udf(lambda z:extract_handles(z),ArrayType(StringType(), True))   \n",
    "# test = extract_handles(\"Hi @MichelleObama , remember when you praised Harvey Weinstein as 'a wonderful human being, a good friend and a powerhouse.\")\n",
    "# print(test)\n",
    "\n",
    "\n",
    "sqlDF = sqlDF.withColumn(\"curated_content\", convert_emojii_UDF(col(\"content\"))) \\\n",
    "                .withColumn(\"tco1_step1_domain\", extract_domain_information_UDF(col(\"tco1_step1\"))) \\\n",
    "                .withColumn(\"tco2_step1_domain\", extract_domain_information_UDF(col(\"tco2_step1\"))) \\\n",
    "                .withColumn(\"tco3_step1_domain\", extract_domain_information_UDF(col(\"tco3_step1\"))) \\\n",
    "                .withColumn(\"handles\", extract_handles_UDF(col(\"content\")))\n",
    "\n",
    "\n",
    "sqlDF.select([\"handles\"]).show(100, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDF.createOrReplaceTempView(\"english_tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_sqlDF = spark.sql(\"SELECT tco1_step1, tco2_step1, tco3_step1  FROM english_tweets LIMIT 10\")\n",
    "_sqlDF.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDF = spark.sql(\"SELECT content,source_file FROM english_tweets LIMIT 100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5110 Spark 3.1",
   "language": "python",
   "name": "ds5110_spark3.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
